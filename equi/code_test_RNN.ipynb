{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hnguyen/nghia_branch/ourproject/equiRL\n"
     ]
    }
   ],
   "source": [
    "# %cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !. ./prepare_1.0.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "from torchsummary import summary\n",
    "import torchvision.models as model_pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TanhWrappedDistribution(torch.distributions.Distribution):\n",
    "    \"\"\"\n",
    "    Class that wraps another valid torch distribution, such that sampled values from the base distribution are\n",
    "    passed through a tanh layer. The corresponding (log) probabilities are also modified accordingly.\n",
    "    Tanh Normal distribution - adapted from rlkit and CQL codebase\n",
    "    (https://github.com/aviralkumar2907/CQL/blob/d67dbe9cf5d2b96e3b462b6146f249b3d6569796/d4rl/rlkit/torch/distributions.py#L6).\n",
    "    \"\"\"\n",
    "    def __init__(self, base_dist, scale=1.0, epsilon=1e-6):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            base_dist (Distribution): Distribution to wrap with tanh output\n",
    "            scale (float): Scale of output\n",
    "            epsilon (float): Numerical stability epsilon when computing log-prob.\n",
    "        \"\"\"\n",
    "        self.base_dist = base_dist\n",
    "        self.scale = scale\n",
    "        self.tanh_epsilon = epsilon\n",
    "        super(TanhWrappedDistribution, self).__init__()\n",
    "\n",
    "    def log_prob(self, value, pre_tanh_value=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            value (torch.Tensor): some tensor to compute log probabilities for\n",
    "            pre_tanh_value: If specified, will not calculate atanh manually from @value. More numerically stable\n",
    "        \"\"\"\n",
    "        value = value / self.scale\n",
    "        if pre_tanh_value is None:\n",
    "            one_plus_x = (1. + value).clamp(min=self.tanh_epsilon)\n",
    "            one_minus_x = (1. - value).clamp(min=self.tanh_epsilon)\n",
    "            pre_tanh_value = 0.5 * torch.log(one_plus_x / one_minus_x)\n",
    "        lp = self.base_dist.log_prob(pre_tanh_value)\n",
    "        tanh_lp = torch.log(1 - value * value + self.tanh_epsilon)\n",
    "        # In case the base dist already sums up the log probs, make sure we do the same\n",
    "        return lp - tanh_lp if len(lp.shape) == len(tanh_lp.shape) else lp - tanh_lp.sum(-1)\n",
    "    def sample(self, sample_shape=torch.Size(), return_pretanh_value=False):\n",
    "        \"\"\"\n",
    "        Gradients will and should *not* pass through this operation.\n",
    "        See https://github.com/pytorch/pytorch/issues/4620 for discussion.\n",
    "        \"\"\"\n",
    "        z = self.base_dist.sample(sample_shape=sample_shape).detach()\n",
    "\n",
    "        if return_pretanh_value:\n",
    "            return torch.tanh(z) * self.scale, z\n",
    "        else:\n",
    "            return torch.tanh(z) * self.scale\n",
    "\n",
    "    def rsample(self, sample_shape=torch.Size(), return_pretanh_value=False):\n",
    "        \"\"\"\n",
    "        Sampling in the reparameterization case - for differentiable samples.\n",
    "        \"\"\"\n",
    "        z = self.base_dist.rsample(sample_shape=sample_shape)\n",
    "\n",
    "        if return_pretanh_value:\n",
    "            return torch.tanh(z) * self.scale, z\n",
    "        else:\n",
    "            return torch.tanh(z) * self.scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BC_RNN_GMM_actor(nn.Module):\n",
    "    def __init__(self, obs_shape = (1,128,128), device = \"cpu\", node = 5):\n",
    "        super(BC_RNN_GMM_actor, self).__init__()\n",
    "        self.channel = obs_shape[0]\n",
    "        self.device = device\n",
    "        self.node = node\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=self.channel, out_channels=32, kernel_size=3, stride =1, padding=1),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride =1, padding = 1),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding = 1),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size = 3, stride =1, padding = 1),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Linear(256*8*8, 512)\n",
    "        self.fc2 = nn.Linear(512,128)\n",
    "        self.fc3 = nn.Linear(128,32)\n",
    "\n",
    "        self.per_step_mean = nn.Linear(200, node*8)\n",
    "        self.per_step_std = nn.Linear(200, node*8)\n",
    "        self.per_step_logit = nn.Linear(200, node)\n",
    "\n",
    "        self.rnn = nn.LSTM(32, 200 , 2, batch_first=True)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self, x, init_state = None, train = True):\n",
    "        # Process the input image with the CNN\n",
    "        length = x.shape[1]\n",
    "        x = x.view(x.size(0)*x.size(1),x.size(2),x.size(3),x.size(4))\n",
    "        x = self.encoder(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Process the CNN output with the LSTM\n",
    "        x = self.relu(self.fc1(x))\n",
    "\n",
    "        x = self.relu(self.fc2(x))\n",
    "\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        x = x.view(-1,length,x.size(1))\n",
    "        \n",
    "        if init_state is None:\n",
    "            h0 = torch.zeros(2,x.size(0),200).to(self.device)\n",
    "            c0 = torch.zeros(2,x.size(0),200).to(self.device)\n",
    "            init_state = (h0,c0)\n",
    "        out, (hn,cn) = self.rnn(x, init_state)\n",
    "\n",
    "        out = out.contiguous().view(-1,200)\n",
    "\n",
    "        mean = self.per_step_mean(out)\n",
    "\n",
    "        mean = mean.view(-1, length, self.node, 8)\n",
    "\n",
    "        std = self.per_step_std(out)\n",
    "        \n",
    "        std = std.view(-1, length, self.node, 8)\n",
    "\n",
    "        logits = self.per_step_logit(out)\n",
    "\n",
    "        logits = logits.view(-1, length, self.node)\n",
    "\n",
    "\n",
    "        if not train:\n",
    "            std = torch.ones_like(mean) * 1e-4\n",
    "        else:\n",
    "            std = torch.nn.functional.softplus(std) + 1e-4\n",
    "\n",
    "        component_distribution = torch.distributions.Normal(loc=mean, scale=std)\n",
    "        component_distribution = torch.distributions.Independent(component_distribution, 1)\n",
    "        mixture_distribution = torch.distributions.Categorical(logits=logits)\n",
    "\n",
    "        dists = torch.distributions.MixtureSameFamily(\n",
    "            mixture_distribution=mixture_distribution,\n",
    "            component_distribution=component_distribution,\n",
    "        )\n",
    "\n",
    "        dists = TanhWrappedDistribution(base_dist=dists, scale=1.)\n",
    "\n",
    "        return dists, hn, cn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BC_RNN_GMM_actor((1,128,128), device = \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn((3,10,1,128,128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hnguyen/miniconda3/envs/softgym/lib/python3.8/site-packages/torch/distributions/distribution.py:45: UserWarning: <class '__main__.TanhWrappedDistribution'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.\n",
      "  warnings.warn(f'{self.__class__} does not define `arg_constraints`. ' +\n"
     ]
    }
   ],
   "source": [
    "dist, hn, cn = model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = dist.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9857)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet50Gray(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet50Gray, self).__init__()\n",
    "        # Load the pre-trained ResNet-50 model\n",
    "        resnet = model_pretrain.resnet50(pretrained=True)\n",
    "        # Modify the first convolutional layer to accept 1 input channel instead of 3\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        # Copy the weights from the original RGB channel to all 3 channels\n",
    "        self.conv1.weight.data[:, :3, :, :] = resnet.conv1.weight.data.clone()\n",
    "        self.conv1.weight.data[:, 3:, :, :] = self.conv1.weight.data[:, :1, :, :]\n",
    "        # Use the rest of the pre-trained ResNet-50 model\n",
    "        self.bn1 = resnet.bn1\n",
    "        self.relu = resnet.relu\n",
    "        self.maxpool = resnet.maxpool\n",
    "        self.layer1 = resnet.layer1\n",
    "        self.layer2 = resnet.layer2\n",
    "        self.layer3 = resnet.layer3\n",
    "        self.layer4 = resnet.layer4\n",
    "        self.avgpool = resnet.avgpool\n",
    "        self.fc = resnet.fc\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convert grayscale images to RGB format\n",
    "        x = x.repeat(1, 3, 1, 1)\n",
    "        # Use the modified first convolutional layer\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hnguyen/miniconda3/envs/softgym/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/hnguyen/miniconda3/envs/softgym/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (1) must match the existing size (3) at non-singleton dimension 1.  Target sizes: [64, 1, 7, 7].  Tensor sizes: [64, 3, 7, 7]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m resnet \u001b[39m=\u001b[39m ResNet50Gray()\n",
      "Cell \u001b[0;32mIn[17], line 9\u001b[0m, in \u001b[0;36mResNet50Gray.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1 \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mConv2d(\u001b[39m1\u001b[39m, \u001b[39m64\u001b[39m, kernel_size\u001b[39m=\u001b[39m\u001b[39m7\u001b[39m, stride\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, padding\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, bias\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m \u001b[39m# Copy the weights from the original RGB channel to all 3 channels\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1\u001b[39m.\u001b[39;49mweight\u001b[39m.\u001b[39;49mdata[:, :\u001b[39m3\u001b[39;49m, :, :] \u001b[39m=\u001b[39m resnet\u001b[39m.\u001b[39mconv1\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mclone()\n\u001b[1;32m     10\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mdata[:, \u001b[39m3\u001b[39m:, :, :] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mdata[:, :\u001b[39m1\u001b[39m, :, :]\n\u001b[1;32m     11\u001b[0m \u001b[39m# Use the rest of the pre-trained ResNet-50 model\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (1) must match the existing size (3) at non-singleton dimension 1.  Target sizes: [64, 1, 7, 7].  Tensor sizes: [64, 3, 7, 7]"
     ]
    }
   ],
   "source": [
    "resnet = ResNet50Gray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_actor(nn.Module):\n",
    "    def __init__(self, input_shape = (1,128,128), device = \"cpu\"):\n",
    "        \n",
    "        super(RNN_actor, self).__init__()\n",
    "        \n",
    "        # Define the CNN\n",
    "        # self.length = input_shape[0]\n",
    "        self.channel = input_shape[0]\n",
    "        self.device = device\n",
    "        # self.length_video = length_video\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=self.channel, out_channels=32, kernel_size=3, stride =1, padding=1),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride =1, padding = 1),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding = 1),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Linear(128*16*16, 512)\n",
    "        self.rnn = nn.RNN(512, 64, 1, batch_first=True)\n",
    "        self.fc2 = nn.Linear(64, 8)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Process the input image with the CNN\n",
    "        length = x.shape[1]\n",
    "        x = x.view(x.size(0)*x.size(1),x.size(2),x.size(3),x.size(4))\n",
    "        x = self.cnn(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Process the CNN output with the LSTM\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        x = x.view(-1,length,x.size(1))\n",
    "        \n",
    "        h0 = torch.zeros(1, x.size(0), 64).to(self.device)\n",
    "\n",
    "        out, _ = self.rnn(x, h0)\n",
    "\n",
    "        out = self.fc2(out)\n",
    "        # Compute the output\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BC_RNN_actor(nn.Module):\n",
    "    def __init__(self, obs_shape = (1,128,128), device = \"cpu\"):\n",
    "        \n",
    "        super(BC_RNN_actor, self).__init__()\n",
    "        \n",
    "        # Define the CNN\n",
    "        # self.length = input_shape[0]\n",
    "        self.channel = obs_shape[0]\n",
    "        self.device = device\n",
    "        # self.picker_shape = picker_shape\n",
    "        # self.length_video = length_video\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=self.channel, out_channels=32, kernel_size=3, stride =1, padding=1),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride =1, padding = 1),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding = 1),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size = 3, stride =1, padding = 1),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Linear(256*8*8, 512)\n",
    "        self.fc2 = nn.Linear(512,128)\n",
    "        self.fc3 = nn.Linear(128,32)\n",
    "        self.rnn = nn.LSTM(32, 200 , 2, batch_first=True)\n",
    "        self.per_step = nn.Linear(200, 8)\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self, x):\n",
    "        # Process the input image with the CNN\n",
    "        length = x.shape[1]\n",
    "        x = x.view(x.size(0)*x.size(1),x.size(2),x.size(3),x.size(4))\n",
    "        x = self.encoder(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Process the CNN output with the LSTM\n",
    "        x = self.relu(self.fc1(x))\n",
    "\n",
    "        x = self.relu(self.fc2(x))\n",
    "\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        x = x.view(-1,length,x.size(1))\n",
    "        \n",
    "\n",
    "        out, _ = self.rnn(x)\n",
    "\n",
    "        out = out.contiguous().view(-1,200)\n",
    "\n",
    "        out = self.per_step(out)\n",
    "\n",
    "        out = out.view(-1, length, out.size(1))\n",
    "\n",
    "        out = torch.tanh(out)\n",
    "\n",
    "        # Compute the output\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BC_RNN_actor((3,128,128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((3,30,3,128,128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.view(1,1,*x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([90, 200])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.contiguous().view(-1,200).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = out.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.1168578 , -0.07547387,  0.09831032,  0.09051739, -0.15409046,\n",
       "       -0.03680436,  0.1875623 ,  0.05464403], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 128, 128]             896\n",
      "         MaxPool2d-2           [-1, 32, 64, 64]               0\n",
      "            Conv2d-3           [-1, 64, 64, 64]          18,496\n",
      "         MaxPool2d-4           [-1, 64, 32, 32]               0\n",
      "            Conv2d-5          [-1, 128, 32, 32]          73,856\n",
      "         MaxPool2d-6          [-1, 128, 16, 16]               0\n",
      "            Linear-7                  [-1, 512]      16,777,728\n",
      "               RNN-8  [[-1, 1, 64], [-1, 2, 64]]               0\n",
      "            Linear-9                 [-1, 1, 8]             520\n",
      "================================================================\n",
      "Total params: 16,871,496\n",
      "Trainable params: 16,871,496\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.19\n",
      "Forward/backward pass size (MB): 8.69\n",
      "Params size (MB): 64.36\n",
      "Estimated Total Size (MB): 73.24\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model,input_size = (1,3,128,128), device = \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "softgym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
